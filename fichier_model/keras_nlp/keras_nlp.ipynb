{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import keras_nlp\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "VOCAB_SIZE = 300000\n",
    "\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM = 512"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../dataset/goodreads_train.csv\")\n",
    "test = pd.read_csv(\"../../dataset/goodreads_test.csv\")\n",
    "vocabulary = np.load('../../vocabulaires/voc_without_std_word_count_5.npy', allow_pickle=True)\n",
    "rating = keras.utils.to_categorical(train['rating'], num_classes=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I love Stephenson - and this was another hit - absolutely loved it. The great thing about a good Stephenson book is it makes you think about the future in new ways, and this book was no exception. \\n It was really two books, and I certainly didn't see the second one coming. It starts out in modern times and then someone blows up the moon. We don't have time to find out who, as within a few years the fragments of the moon cause the worst asteriod shower earth has ever seen and wipe out all life in earth. We have time to send 1,500 people up into space - and this is their story. \\n The use of robots throughout the books was fascinating to me. Stephenson has clearly looked 10-20 years into our future and correctly predicted how it will go. From robot workers in space, nano-bots, nano-robot weapons, and more - we get a vivid portrayal of how robots might be a part of our future lives. \\n I thought the focus on use of whip technology in space was interesting. And of course, the whole notion of Cradle was just cool - though not sure about it's feasibility.\""
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "voc = []\n",
    "with open(\"word_piece_vocabulary\") as f:\n",
    "    for line in f:\n",
    "        voc.append(line[:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "one_data = train['review_text'][5:15]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 312)\n",
      "(None, None, 312, 300)\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
    "vectorize_layer = keras_nlp.tokenizers.WordPieceTokenizer(voc, 312, lowercase=True,\n",
    "                                                          strip_accents=True)(inputs)\n",
    "print(vectorize_layer.shape)\n",
    "#vectorize_layer = keras.layers.Lambda(low_dim)(vectorize_layer)\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(len(voc), 312, 300)(vectorize_layer)\n",
    "print(x.shape)\n",
    "model = keras.Model(inputs=[inputs], outputs=vectorize_layer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 610ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10, 1, 312)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.predict(one_data)\n",
    "np.shape(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "            standardize='lower_and_strip_punctuation',\n",
    "            split='whitespace',\n",
    "            output_mode='int',\n",
    "            output_sequence_length=312,\n",
    "            vocabulary=voc\n",
    "        )(inputs)\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(len(voc), 312, 300)(vectorize_layer)\n",
    "model = keras.Model(inputs=[inputs], outputs=x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 168ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10, 312, 300)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.predict(one_data)\n",
    "np.shape(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices(train['review_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(data, 300000, vocabulary_output_file='word_piece_vocabulary',lowercase=True, strip_accents=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "token = keras_nlp.tokenizers.WordPieceTokenizer('word_piece_vocabulary',300,lowercase=True, strip_accents=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras_nlp.tokenizers.word_piece_tokenizer.WordPieceTokenizer at 0x1a5a48555a0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# def train_word_piece(ds, vocab_size, reserved_tokens):\n",
    "#     #word_piece_ds = ds.unbatch().map(lambda x, y: x)\n",
    "#     vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "#         ds.tolist(),\n",
    "#         vocabulary_size=vocab_size,\n",
    "#         reserved_tokens=reserved_tokens,\n",
    "#     )\n",
    "#     return vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# reserved_tokens = [\"[PAD]\", \"[UNK]\"]\n",
    "# train_sentences = [element[0] for element in train['review_text']]\n",
    "# vocab = train_word_piece(train['review_text'], VOCAB_SIZE, reserved_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# print(\"Tokens: \", vocab[100:110])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "#     vocabulary=vocab,\n",
    "#     lowercase=False,\n",
    "#     sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# input_sentence_ex = train_ds.take(1).get_single_element()[0][0]\n",
    "# input_tokens_ex = tokenizer(input_sentence_ex)\n",
    "#\n",
    "# print(\"Sentence: \", input_sentence_ex)\n",
    "# print(\"Tokens: \", input_tokens_ex)\n",
    "# print(\"Recovered text after detokenizing: \", tokenizer.detokenize(input_tokens_ex))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
    "inputs1 = keras.Input(shape=(1,), dtype=tf.string)\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "            standardize='lower_and_strip_punctuation',\n",
    "            split='whitespace',\n",
    "            output_mode='int',\n",
    "            output_sequence_length=512,\n",
    "            vocabulary=vocabulary\n",
    "        )(inputs1)\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(vectorize_layer)\n",
    "\n",
    "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
    "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
    "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
    "\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "outputs = keras.layers.Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "fnet_classifier = keras.Model(inputs1, outputs, name=\"fnet_classifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fnet_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 512)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 512, 128)         38465536  \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " f_net_encoder (FNetEncoder)  (None, 512, 128)         132224    \n",
      "                                                                 \n",
      " f_net_encoder_1 (FNetEncode  (None, 512, 128)         132224    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " f_net_encoder_2 (FNetEncode  (None, 512, 128)         132224    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,862,982\n",
      "Trainable params: 38,862,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1440/1440 [==============================] - 296s 204ms/step - loss: 1.2217 - categorical_accuracy: 0.4781 - f1_score: 0.4601 - val_loss: 1.0394 - val_categorical_accuracy: 0.5545 - val_f1_score: 0.5520\n",
      "Epoch 2/6\n",
      "1440/1440 [==============================] - 301s 209ms/step - loss: 0.9968 - categorical_accuracy: 0.5739 - f1_score: 0.5703 - val_loss: 0.9793 - val_categorical_accuracy: 0.5797 - val_f1_score: 0.5753\n",
      "Epoch 3/6\n",
      "1440/1440 [==============================] - 320s 222ms/step - loss: 0.9423 - categorical_accuracy: 0.5993 - f1_score: 0.5969 - val_loss: 0.9662 - val_categorical_accuracy: 0.5863 - val_f1_score: 0.5838\n",
      "Epoch 4/6\n",
      "1440/1440 [==============================] - 299s 208ms/step - loss: 0.8974 - categorical_accuracy: 0.6198 - f1_score: 0.6179 - val_loss: 0.9647 - val_categorical_accuracy: 0.5890 - val_f1_score: 0.5867\n",
      "Epoch 5/6\n",
      "1440/1440 [==============================] - 300s 208ms/step - loss: 0.8531 - categorical_accuracy: 0.6412 - f1_score: 0.6397 - val_loss: 0.9862 - val_categorical_accuracy: 0.5864 - val_f1_score: 0.5853\n",
      "Epoch 6/6\n",
      "1440/1440 [==============================] - 301s 209ms/step - loss: 0.8057 - categorical_accuracy: 0.6638 - f1_score: 0.6625 - val_loss: 0.9985 - val_categorical_accuracy: 0.5847 - val_f1_score: 0.5830\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x24eb86d9960>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnet_classifier.summary()\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "if not os.path.exists(f\"logs/fnet_classifier\"):\n",
    "    os.mkdir(f\"logs/fnet_classifier\")\n",
    "if not os.path.exists(f\"checkpoint/fnet_classifier\"):\n",
    "    os.mkdir(f\"checkpoint/fnet_classifier\")\n",
    "\n",
    "chekpoint = keras.callbacks.ModelCheckpoint(f'checkpoint/fnet_classifier/', save_weights_only=True,\n",
    "monitor='val_f1_score',\n",
    "mode='max',\n",
    "save_best_only=True)\n",
    "callbacks = []\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs/fnet_classifier\")\n",
    "\n",
    "fnet_classifier.compile(optimizer=keras.optimizers.Adamax(),\n",
    "                           loss=keras.losses.categorical_crossentropy,\n",
    "                           metrics=[keras.metrics.categorical_accuracy,\n",
    "                                    tfa.metrics.F1Score(num_classes=6, average='weighted')]\n",
    "                           )\n",
    "fnet_classifier.fit(train['review_text'], rating, epochs=6, batch_size=500,\n",
    "                                 validation_split=0.2, #class_weight=class_weight,\n",
    "                                 callbacks=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
