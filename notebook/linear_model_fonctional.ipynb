{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open dataset\n"
     ]
    }
   ],
   "source": [
    "#read train dataset\n",
    "print(\"open dataset\")\n",
    "train = pd.read_csv(\"../dataset/goodreads_train.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load a np_archive of review_text col of train dataset preprocess in main.py\n",
    "train_prepro = pd.DataFrame(data=np.load(file=\"../vocabulaires/prepro_train_archive_PN_less.npy\", allow_pickle=True), columns=['review_text'])['review_text']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# add review text col to train dataset\n",
    "train['review_text'] = train_prepro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   30988\n",
      "1   28718\n",
      "2   72627\n",
      "3   188972\n",
      "4   313688\n",
      "5   265007\n"
     ]
    }
   ],
   "source": [
    "print(\"0  \",train[train['rating'] == 0][\"rating\"].count())\n",
    "print(\"1  \", train[train['rating'] == 1][\"rating\"].count())\n",
    "print(\"2  \", train[train['rating'] == 2][\"rating\"].count())\n",
    "print(\"3  \", train[train['rating'] == 3][\"rating\"].count())\n",
    "print(\"4  \", train[train['rating'] == 4][\"rating\"].count())\n",
    "print(\"5  \", train[train['rating'] == 5][\"rating\"].count())\n",
    "#train_3_5 = train[train['rating'] >= 3]\n",
    "#train['rating'] = train['rating'].apply(lambda x: x-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "rating = keras.utils.to_categorical(train['rating'], num_classes=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#create model layers\n",
    "inputs = keras.Input(shape=(1,), dtype=tf.string) # text\n",
    "inputs2 = keras.Input(shape=(1), dtype=tf.int64) # n_comment\n",
    "inputs3 = keras.Input(shape=(1), dtype=tf.int64) # n_votes\n",
    "#create vectorize layer, to transform words in integer\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=1400,\n",
    "    vocabulary=np.load('../vocabulaires/voc_lemm_without_NP.npy')\n",
    ")(inputs)\n",
    "conc = keras.layers.concatenate([vectorize_layer, inputs2,inputs3])\n",
    "outputs = keras.layers.Dense(6, activation=tf.keras.activations.tanh)(conc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[inputs, inputs2, inputs3], outputs=outputs, name=\"mnist_model\")\n",
    "tensorboard = TensorBoard(log_dir=\"../logs/relu\".format(time()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#learning_rates = [0.01, 0.001, 0.000001, 0.0000001]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 25s 3s/step - loss: 6.3555 - categorical_accuracy: 0.0913\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 24s 3s/step - loss: 4.7275 - categorical_accuracy: 0.1183\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 23s 3s/step - loss: 3.7915 - categorical_accuracy: 0.1149\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 23s 3s/step - loss: 3.3351 - categorical_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 23s 3s/step - loss: 3.0892 - categorical_accuracy: 0.0874\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x22a88b52050>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for learning_rate in learning_rates:\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy]\n",
    "              )\n",
    "model.fit([train['review_text'], train['n_comments'], train['n_votes']], rating, epochs=5,\n",
    "                  callbacks=[\n",
    "                      tf.keras.callbacks.TensorBoard(log_dir=\"../logs/relu\"),\n",
    "                  ],\n",
    "                  batch_size=100000\n",
    "                  )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#model.save(\"../models_trained/linear_model_2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#read test dataset\n",
    "test = pd.read_csv(\"../dataset/goodreads_test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load a np_archive of review_text col of train dataset preprocess in main.py\n",
    "test_prepro = pd.DataFrame(data=np.load(file=\"../vocabulaires/prepro_test_archive_PN_less.npy\", allow_pickle=True), columns=['review_text'])['review_text']\n",
    "test['review_text'] = test_prepro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 user_id   book_id  \\\n0       b9450d1c1f97f891c392b1105959b56e   7092507   \n1       b9450d1c1f97f891c392b1105959b56e   5576654   \n2       b9450d1c1f97f891c392b1105959b56e  15754052   \n3       b9450d1c1f97f891c392b1105959b56e     17020   \n4       b9450d1c1f97f891c392b1105959b56e  12551082   \n...                                  ...       ...   \n478028  35cef391b171b4fca45771e508028212  15745950   \n478029  35cef391b171b4fca45771e508028212  10861195   \n478030  35cef391b171b4fca45771e508028212   6131164   \n478031  35cef391b171b4fca45771e508028212  10025305   \n478032  35cef391b171b4fca45771e508028212   6482837   \n\n                               review_id  \\\n0       5c4df7e70e9b438c761f07a4620ccb7c   \n1       8eaeaf13213eeb16ad879a2a2591bbe5   \n2       dce649b733c153ba5363a0413cac988f   \n3       8a46df0bb997269d6834f9437a4b0a77   \n4       d11d3091e22f1cf3cb865598de197599   \n...                                  ...   \n478028  0e1db3d4b04256f9660f5d276ddf1314   \n478029  0b7f352e58caf0fd1f961e98ef04e89c   \n478030  9b19eff33ddb14e9e68fca2e90379e46   \n478031  8be463fed78f0da63e964706f710332b   \n478032  62ed1263c7d216986cc419cd4e8a408b   \n\n                                              review_text  \\\n0       * spoiler alert this definitely one favorite a...   \n1       * spoiler alert `` you drink . '' i 'm huge fa...   \n2       one favorite character under i 'm happy read s...   \n3       * spoiler alert if feel like travelling n't mo...   \n4       3.5 star i read enjoyed first two novel series...   \n...                                                   ...   \n478028                    n't wait ' before ... after ...   \n478029  to-read shelf forever . update i 've finished ...   \n478030  the last book left wanting . i need happy endi...   \n478031  things heating second novel devices . will beg...   \n478032  before i even start review , i must say word t...   \n\n                            date_added                    date_updated  \\\n0       Sat Nov 10 06:06:13 -0800 2012  Sun Nov 11 05:38:36 -0800 2012   \n1       Fri Nov 09 21:55:16 -0800 2012  Sat Nov 10 05:41:49 -0800 2012   \n2       Fri Nov 09 00:25:50 -0800 2012  Sat Nov 10 06:14:10 -0800 2012   \n3       Thu Nov 01 00:28:39 -0700 2012  Sat Nov 03 11:35:22 -0700 2012   \n4       Thu Oct 18 00:57:00 -0700 2012  Mon Apr 01 23:00:51 -0700 2013   \n...                                ...                             ...   \n478028  Sun Aug 05 10:26:12 -0700 2012  Tue Apr 16 17:24:00 -0700 2013   \n478029  Tue Jul 10 23:31:00 -0700 2012  Fri Dec 28 20:05:51 -0800 2012   \n478030  Tue Jul 10 19:45:17 -0700 2012  Mon Mar 25 18:41:51 -0700 2013   \n478031  Thu Jul 05 19:19:30 -0700 2012  Thu Jan 24 16:24:54 -0800 2013   \n478032  Mon Jun 04 18:06:26 -0700 2012  Sat Dec 29 17:47:56 -0800 2012   \n\n                               read_at                      started_at  \\\n0       Sun Nov 11 05:38:36 -0800 2012  Sat Nov 10 00:00:00 -0800 2012   \n1       Sat Nov 10 05:41:49 -0800 2012  Fri Nov 09 00:00:00 -0800 2012   \n2       Sat Nov 10 06:14:10 -0800 2012  Fri Nov 09 00:00:00 -0800 2012   \n3       Sat Nov 03 11:35:22 -0700 2012  Thu Nov 01 00:00:00 -0700 2012   \n4       Sat Mar 30 00:00:00 -0700 2013  Fri Mar 29 00:00:00 -0700 2013   \n...                                ...                             ...   \n478028  Tue Apr 16 00:00:00 -0700 2013                             NaN   \n478029                             NaN                             NaN   \n478030  Tue Mar 19 00:00:00 -0700 2013                             NaN   \n478031  Mon Jan 14 00:00:00 -0800 2013                             NaN   \n478032                             NaN                             NaN   \n\n        n_votes  n_comments  \n0             1           0  \n1             1           0  \n2             0           0  \n3             0           0  \n4             0           0  \n...         ...         ...  \n478028        0           0  \n478029        0           0  \n478030        0           0  \n478031        0           0  \n478032        0           0  \n\n[478033 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>7092507</td>\n      <td>5c4df7e70e9b438c761f07a4620ccb7c</td>\n      <td>* spoiler alert this definitely one favorite a...</td>\n      <td>Sat Nov 10 06:06:13 -0800 2012</td>\n      <td>Sun Nov 11 05:38:36 -0800 2012</td>\n      <td>Sun Nov 11 05:38:36 -0800 2012</td>\n      <td>Sat Nov 10 00:00:00 -0800 2012</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>5576654</td>\n      <td>8eaeaf13213eeb16ad879a2a2591bbe5</td>\n      <td>* spoiler alert `` you drink . '' i 'm huge fa...</td>\n      <td>Fri Nov 09 21:55:16 -0800 2012</td>\n      <td>Sat Nov 10 05:41:49 -0800 2012</td>\n      <td>Sat Nov 10 05:41:49 -0800 2012</td>\n      <td>Fri Nov 09 00:00:00 -0800 2012</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>15754052</td>\n      <td>dce649b733c153ba5363a0413cac988f</td>\n      <td>one favorite character under i 'm happy read s...</td>\n      <td>Fri Nov 09 00:25:50 -0800 2012</td>\n      <td>Sat Nov 10 06:14:10 -0800 2012</td>\n      <td>Sat Nov 10 06:14:10 -0800 2012</td>\n      <td>Fri Nov 09 00:00:00 -0800 2012</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>17020</td>\n      <td>8a46df0bb997269d6834f9437a4b0a77</td>\n      <td>* spoiler alert if feel like travelling n't mo...</td>\n      <td>Thu Nov 01 00:28:39 -0700 2012</td>\n      <td>Sat Nov 03 11:35:22 -0700 2012</td>\n      <td>Sat Nov 03 11:35:22 -0700 2012</td>\n      <td>Thu Nov 01 00:00:00 -0700 2012</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b9450d1c1f97f891c392b1105959b56e</td>\n      <td>12551082</td>\n      <td>d11d3091e22f1cf3cb865598de197599</td>\n      <td>3.5 star i read enjoyed first two novel series...</td>\n      <td>Thu Oct 18 00:57:00 -0700 2012</td>\n      <td>Mon Apr 01 23:00:51 -0700 2013</td>\n      <td>Sat Mar 30 00:00:00 -0700 2013</td>\n      <td>Fri Mar 29 00:00:00 -0700 2013</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>478028</th>\n      <td>35cef391b171b4fca45771e508028212</td>\n      <td>15745950</td>\n      <td>0e1db3d4b04256f9660f5d276ddf1314</td>\n      <td>n't wait ' before ... after ...</td>\n      <td>Sun Aug 05 10:26:12 -0700 2012</td>\n      <td>Tue Apr 16 17:24:00 -0700 2013</td>\n      <td>Tue Apr 16 00:00:00 -0700 2013</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>478029</th>\n      <td>35cef391b171b4fca45771e508028212</td>\n      <td>10861195</td>\n      <td>0b7f352e58caf0fd1f961e98ef04e89c</td>\n      <td>to-read shelf forever . update i 've finished ...</td>\n      <td>Tue Jul 10 23:31:00 -0700 2012</td>\n      <td>Fri Dec 28 20:05:51 -0800 2012</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>478030</th>\n      <td>35cef391b171b4fca45771e508028212</td>\n      <td>6131164</td>\n      <td>9b19eff33ddb14e9e68fca2e90379e46</td>\n      <td>the last book left wanting . i need happy endi...</td>\n      <td>Tue Jul 10 19:45:17 -0700 2012</td>\n      <td>Mon Mar 25 18:41:51 -0700 2013</td>\n      <td>Tue Mar 19 00:00:00 -0700 2013</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>478031</th>\n      <td>35cef391b171b4fca45771e508028212</td>\n      <td>10025305</td>\n      <td>8be463fed78f0da63e964706f710332b</td>\n      <td>things heating second novel devices . will beg...</td>\n      <td>Thu Jul 05 19:19:30 -0700 2012</td>\n      <td>Thu Jan 24 16:24:54 -0800 2013</td>\n      <td>Mon Jan 14 00:00:00 -0800 2013</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>478032</th>\n      <td>35cef391b171b4fca45771e508028212</td>\n      <td>6482837</td>\n      <td>62ed1263c7d216986cc419cd4e8a408b</td>\n      <td>before i even start review , i must say word t...</td>\n      <td>Mon Jun 04 18:06:26 -0700 2012</td>\n      <td>Sat Dec 29 17:47:56 -0800 2012</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>478033 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 80s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# test the model with test dataset\n",
    "res = model.predict([train['review_text'], train['n_comments'], train['n_votes']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1., -1., -1., -1., -1., -1.],\n       [-1., -1., -1., -1., -1., -1.],\n       [-1., -1., -1., -1., -1.,  1.],\n       ...,\n       [-1., -1., -1., -1., -1., -1.],\n       [-1., -1., -1., -1., -1., -1.],\n       [-1., -1., -1., -1., -1., -1.]], dtype=float32)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14939/14939 [==============================] - 36s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "restest = model.predict([test['review_text'], test['n_comments'], test['n_votes']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/478033 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8379d49e7af04df0b922d3a66efb1cc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([0, 0, 5, ..., 5, 0, 0])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse keras.utils.to_categorical for kaggle submission\n",
    "ff = []\n",
    "for line in tqdm(restest):\n",
    "    tmp = -2\n",
    "    category = None\n",
    "    for i in (range(6)):\n",
    "        if line[i] > tmp:\n",
    "            category = i\n",
    "            tmp = line[i]\n",
    "    ff.append(category)\n",
    "data = np.array(ff)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "test['rating'] = data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# create a dataframe for kaggle\n",
    "id = test['review_id'].to_numpy()\n",
    "rating = test['rating'].to_numpy()\n",
    "df = pd.DataFrame( columns=['review_id', 'rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "df['review_id'] = id\n",
    "df['rating'] = rating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# create a csv for submission\n",
    "df.to_csv('submission_linear_model.csv',index=False )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
